{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1512919,"sourceType":"datasetVersion","datasetId":611716}],"dockerImageVersionId":30527,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ocular Disease Recognition - Model Training\n\n2023-08-07\n\n## 1. Overview & Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Ocular Disease Intelligent Recognition (ODIR) is a structured ophthalmic database of 5,000 patients with age, color fundus photographs from left and right eyes and doctors' diagnostic keywords from doctors.\n\nThis dataset is meant to represent ‘‘real-life’’ set of patient information collected by Shanggong Medical Technology Co., Ltd. from different hospitals/medical centers in China. In these institutions, fundus images are captured by various cameras in the market, such as Canon, Zeiss and Kowa, resulting into varied image resolutions.\nAnnotations were labeled by trained human readers with quality control management. They classify patient into eight labels including:\n\n- Normal (N),\n- Diabetes (D),\n- Glaucoma (G),\n- Cataract (C),\n- Age related Macular Degeneration (A),\n- Hypertension (H),\n- Pathological Myopia (M),\n- Other diseases/abnormalities (O)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../data/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport torch\ntorch.__version__","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-31T20:42:33.054114Z","iopub.execute_input":"2023-08-31T20:42:33.055015Z","iopub.status.idle":"2023-08-31T20:42:33.063139Z","shell.execute_reply.started":"2023-08-31T20:42:33.054979Z","shell.execute_reply":"2023-08-31T20:42:33.061991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1 Exploratory data analysis\n\nAn issue with the data is that the diagnosis encodings in the one-hot-encoded columns (columns labeled C, D, G...), in particular the numeric encoded diagnosis are incorrect. However the `target` field is correct for the eye referenced in `filename` and matched the diagnostic keywords for the respective eye.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ocular-disease-recognition-odir5k/full_df.csv')\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:33.065111Z","iopub.execute_input":"2023-08-31T20:42:33.065965Z","iopub.status.idle":"2023-08-31T20:42:33.135399Z","shell.execute_reply.started":"2023-08-31T20:42:33.065929Z","shell.execute_reply":"2023-08-31T20:42:33.134473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Problems with the encoded categories\n\nWe found that relying on the encoded diagnosis categories \"N D G C A H M O\" could be problematic in the sense that sthey are incorrect. The problem with this is that the files and diagnosis are orientation specific and the row of data contains both eyes information.\n\nTherefore we will exclude those categories for this notebook although they could certainly be used for other investigations.","metadata":{}},{"cell_type":"code","source":"raw_data = df.drop(columns=['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O', 'labels'])\n\nraw_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:33.137258Z","iopub.execute_input":"2023-08-31T20:42:33.137937Z","iopub.status.idle":"2023-08-31T20:42:33.158225Z","shell.execute_reply.started":"2023-08-31T20:42:33.137899Z","shell.execute_reply":"2023-08-31T20:42:33.157167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of classes will correspond to the number of unique lables in the `labels` column, this data set contains 8 classes corresponding the the diagnosis in the list above","metadata":{}},{"cell_type":"code","source":"raw_data[\"target\"].unique().size","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:33.159779Z","iopub.execute_input":"2023-08-31T20:42:33.160225Z","iopub.status.idle":"2023-08-31T20:42:33.167589Z","shell.execute_reply.started":"2023-08-31T20:42:33.16019Z","shell.execute_reply":"2023-08-31T20:42:33.166624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:33.170654Z","iopub.execute_input":"2023-08-31T20:42:33.171679Z","iopub.status.idle":"2023-08-31T20:42:33.179782Z","shell.execute_reply.started":"2023-08-31T20:42:33.17164Z","shell.execute_reply":"2023-08-31T20:42:33.178629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Plot the number of samples per class","metadata":{}},{"cell_type":"code","source":"targets = np.array(raw_data[\"target\"].apply(lambda x: json.loads(x)).tolist())\n\n\ntargets.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:33.181315Z","iopub.execute_input":"2023-08-31T20:42:33.182261Z","iopub.status.idle":"2023-08-31T20:42:33.235147Z","shell.execute_reply.started":"2023-08-31T20:42:33.182223Z","shell.execute_reply":"2023-08-31T20:42:33.233961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = { 0: \"Normal\",\n            1: \"Diabetes\",\n            2: \"Glaucoma\",\n            3: \"Cataract\",\n            4: \"Age related Macular Degeneration\",\n            5: \"Hypertension\",\n            6: \"Pathological Myopia\",\n            7: \"Other diseases/abnormalities\"\n          }","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:33.23874Z","iopub.execute_input":"2023-08-31T20:42:33.239079Z","iopub.status.idle":"2023-08-31T20:42:33.24538Z","shell.execute_reply.started":"2023-08-31T20:42:33.239049Z","shell.execute_reply":"2023-08-31T20:42:33.244259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the dataset\ndata = np.sum(targets, axis=0)\n\nclasses_names = list(classes.values())\nvalues = list(data)\n  \nfig = plt.figure(figsize = (10, 5))\n \n# creating the bar plot\nplt.bar(classes_names, values, color ='maroon',\n        width = 0.4)\n \nplt.xlabel(\"Diagnosis\")\nplt.ylabel(\"# of samples\")\nplt.title(\"Size of the classes in data set\")\n\nplt.xticks(rotation=90)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:33.246918Z","iopub.execute_input":"2023-08-31T20:42:33.247845Z","iopub.status.idle":"2023-08-31T20:42:33.748966Z","shell.execute_reply.started":"2023-08-31T20:42:33.24781Z","shell.execute_reply":"2023-08-31T20:42:33.747903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that the dataset is imbalanced however there are more that 100 sample images for each class therefore there should be enough data per class to train a classification problem.\n\n### 1.4 Plotting example of each class","metadata":{}},{"cell_type":"code","source":"img_dir = \"/kaggle/input/ocular-disease-recognition-odir5k/preprocessed_images\"","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:33.750521Z","iopub.execute_input":"2023-08-31T20:42:33.750889Z","iopub.status.idle":"2023-08-31T20:42:33.755853Z","shell.execute_reply.started":"2023-08-31T20:42:33.750857Z","shell.execute_reply":"2023-08-31T20:42:33.754529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data[\"class_name\"] = np.argmax(targets, axis=1).tolist()\nraw_data[\"class_name\"] = raw_data[\"class_name\"] .replace(classes)\n\nraw_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:33.757029Z","iopub.execute_input":"2023-08-31T20:42:33.758105Z","iopub.status.idle":"2023-08-31T20:42:33.788641Z","shell.execute_reply.started":"2023-08-31T20:42:33.758071Z","shell.execute_reply":"2023-08-31T20:42:33.787559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 12))\n\nfor i, class_name in enumerate(classes.values()):\n    \n    class_subset = raw_data.loc[raw_data.class_name == class_name].sample(n=4)\n    \n    img_names = class_subset.filename.to_list()\n    \n    for j, img in enumerate(img_names):\n        image = cv2.imread(os.path.join(img_dir, img))\n        \n        ## convert image to RGB\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        ## subplot variables - (# of rows, # of columns, iterate through locations on grid)\n        plt.subplot(8, 4, 4 * i + j + 1)\n        plt.imshow(image_rgb, aspect=1)\n        \n        ## label with filename and diagnosis\n        plt.xlabel('{}'.format(class_name))\n\n        plt.tight_layout() ","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:33.793328Z","iopub.execute_input":"2023-08-31T20:42:33.793741Z","iopub.status.idle":"2023-08-31T20:42:49.49179Z","shell.execute_reply.started":"2023-08-31T20:42:33.7937Z","shell.execute_reply":"2023-08-31T20:42:49.490759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 12))\n\nsingle_class = [\"Glaucoma\"]\n\nfor i, class_name in enumerate(single_class):\n    \n    class_subset = raw_data.loc[raw_data.class_name == class_name].sample(n=4)\n    \n    img_names = class_subset.filename.to_list()\n    \n    for j, img in enumerate(img_names):\n        image = cv2.imread(os.path.join(img_dir, img))\n        \n        ## convert image to RGB\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        ## subplot variables - (# of rows, # of columns, iterate through locations on grid)\n        # plt.subplot(8, 4, 4 * i + j + 1)\n        plt.imshow(image_rgb, aspect=1)\n        \n        ## label with filename and diagnosis\n        plt.xlabel('{}'.format(class_name))","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:49.493438Z","iopub.execute_input":"2023-08-31T20:42:49.494111Z","iopub.status.idle":"2023-08-31T20:42:51.229097Z","shell.execute_reply.started":"2023-08-31T20:42:49.494071Z","shell.execute_reply":"2023-08-31T20:42:51.228075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.5 Saving Cleaned Labels","metadata":{}},{"cell_type":"code","source":"processed_labels = raw_data[[\"ID\", \"filename\", \"class_name\", \"target\"]]\nprocessed_labels.to_csv('/kaggle/working/labels_clean.csv', index=False)\n\nprocessed_labels.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:51.230555Z","iopub.execute_input":"2023-08-31T20:42:51.23136Z","iopub.status.idle":"2023-08-31T20:42:51.281904Z","shell.execute_reply.started":"2023-08-31T20:42:51.231324Z","shell.execute_reply":"2023-08-31T20:42:51.280643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Import Libraries","metadata":{}},{"cell_type":"code","source":"# Import libraries\nimport json\nimport os\nimport pickle\nimport random\nimport time\n\n# Ignore warnings\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch\n\n# PyTorch model\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom skimage import io, transform\nfrom sklearn.metrics import classification_report, confusion_matrix, jaccard_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# PyTorch dataset\nfrom torchvision import datasets, models, transforms, utils\nfrom torchvision.utils import make_grid\n\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()  # interactive mode\n\nfrom __future__ import print_function, division\n\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:51.283964Z","iopub.execute_input":"2023-08-31T20:42:51.28457Z","iopub.status.idle":"2023-08-31T20:42:51.305991Z","shell.execute_reply.started":"2023-08-31T20:42:51.284534Z","shell.execute_reply":"2023-08-31T20:42:51.304642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:51.308186Z","iopub.execute_input":"2023-08-31T20:42:51.308635Z","iopub.status.idle":"2023-08-31T20:42:51.316698Z","shell.execute_reply.started":"2023-08-31T20:42:51.308581Z","shell.execute_reply":"2023-08-31T20:42:51.315643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:51.31807Z","iopub.execute_input":"2023-08-31T20:42:51.319089Z","iopub.status.idle":"2023-08-31T20:42:52.40669Z","shell.execute_reply.started":"2023-08-31T20:42:51.319054Z","shell.execute_reply":"2023-08-31T20:42:52.405418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 Define helper functions","metadata":{}},{"cell_type":"code","source":"def test_network(net, trainloader):\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(net.parameters(), lr=0.001)\n\n    dataiter = iter(trainloader)\n    images, labels = dataiter.next()\n\n    # Create Variables for the inputs and targets\n    inputs = Variable(images)\n    targets = Variable(images)\n\n    # Clear the gradients from all Variables\n    optimizer.zero_grad()\n\n    # Forward pass, then backward pass, then update weights\n    output = net.forward(inputs)\n    loss = criterion(output, targets)\n    loss.backward()\n    optimizer.step()\n\n    return True\n\n\ndef imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax\n\n\ndef view_recon(img, recon):\n    ''' Function for displaying an image (as a PyTorch Tensor) and its\n        reconstruction also a PyTorch Tensor\n    '''\n\n    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n    axes[0].imshow(img.numpy().squeeze())\n    axes[1].imshow(recon.data.numpy().squeeze())\n    for ax in axes:\n        ax.axis('off')\n        ax.set_adjustable('box-forced')\n\ndef view_classify(img, ps, version=\"MNIST\"):\n    ''' Function for viewing an image and it's predicted classes.\n    '''\n    ps = ps.data.numpy().squeeze()\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n    ax1.axis('off')\n    ax2.barh(np.arange(10), ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(np.arange(10))\n    if version == \"MNIST\":\n        ax2.set_yticklabels(np.arange(10))\n    elif version == \"Fashion\":\n        ax2.set_yticklabels(['T-shirt/top',\n                            'Trouser',\n                            'Pullover',\n                            'Dress',\n                            'Coat',\n                            'Sandal',\n                            'Shirt',\n                            'Sneaker',\n                            'Bag',\n                            'Ankle Boot'], size='small');\n    ax2.set_title('Class Probability')\n    ax2.set_xlim(0, 1.1)\n\n    plt.tight_layout()\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:52.40907Z","iopub.execute_input":"2023-08-31T20:42:52.409606Z","iopub.status.idle":"2023-08-31T20:42:52.428305Z","shell.execute_reply.started":"2023-08-31T20:42:52.40954Z","shell.execute_reply":"2023-08-31T20:42:52.427283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_images(image):\n    \"\"\"Show image\"\"\"\n    plt.imshow(image)\n    plt.pause(0.001)  # pause a bit so that plots are updated","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:52.430333Z","iopub.execute_input":"2023-08-31T20:42:52.430751Z","iopub.status.idle":"2023-08-31T20:42:52.44405Z","shell.execute_reply.started":"2023-08-31T20:42:52.430711Z","shell.execute_reply":"2023-08-31T20:42:52.44307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Define `OcularDiseaseRecognition` class","metadata":{}},{"cell_type":"code","source":"class OcularDiseaseRecognition(Dataset):\n    \"\"\"Ocular Disease Recognition.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Arguments:\n            csv_file (string): Path to the csv file with labels.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.labels_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels_frame)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir,\n                                self.labels_frame.iloc[idx, 1])\n        image = io.imread(img_name)\n        target = self.labels_frame.iloc[idx, 3]\n        target = np.array(json.loads(target))\n        sample = {'image': image, 'labels': target}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n    \nclass Rescale(object):\n    \"\"\"Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['labels']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = transform.resize(image, (new_h, new_w))\n\n        return {'image': img, 'labels': label}\n\n\nclass RandomCrop(object):\n    \"\"\"Crop randomly the image in a sample.\n\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n            is made.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['labels']\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w]\n\n\n        return {'image': image, 'labels': label}\n\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['labels']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C x H x W\n        image = image.transpose((2, 0, 1))\n        return {'image': torch.from_numpy(image),\n                'labels': torch.from_numpy(label)}","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:52.447555Z","iopub.execute_input":"2023-08-31T20:42:52.447873Z","iopub.status.idle":"2023-08-31T20:42:52.473182Z","shell.execute_reply.started":"2023-08-31T20:42:52.447847Z","shell.execute_reply":"2023-08-31T20:42:52.472212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Define Pytorch `Dataloader` & Load Data","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Load Labels","metadata":{}},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:52.474842Z","iopub.execute_input":"2023-08-31T20:42:52.475316Z","iopub.status.idle":"2023-08-31T20:42:53.456628Z","shell.execute_reply.started":"2023-08-31T20:42:52.47528Z","shell.execute_reply":"2023-08-31T20:42:53.455435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = pd.read_csv('/kaggle/working/labels_clean.csv')\n\nn = 65\nimg_name = labels.iloc[n, 1]\nclass_name =  labels.iloc[n, 2]\ntarget = labels.iloc[n, 3]\n\nprint('Image name: {}'.format(img_name))\nprint('Class Name: {}'.format(class_name))\nprint('Target: {}'.format(target))","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.459847Z","iopub.execute_input":"2023-08-31T20:42:53.460895Z","iopub.status.idle":"2023-08-31T20:42:53.485181Z","shell.execute_reply.started":"2023-08-31T20:42:53.460852Z","shell.execute_reply":"2023-08-31T20:42:53.484256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.486659Z","iopub.execute_input":"2023-08-31T20:42:53.487015Z","iopub.status.idle":"2023-08-31T20:42:53.498798Z","shell.execute_reply.started":"2023-08-31T20:42:53.486981Z","shell.execute_reply":"2023-08-31T20:42:53.497621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.500525Z","iopub.execute_input":"2023-08-31T20:42:53.501775Z","iopub.status.idle":"2023-08-31T20:42:53.512142Z","shell.execute_reply.started":"2023-08-31T20:42:53.501736Z","shell.execute_reply":"2023-08-31T20:42:53.510555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Load Image Data","metadata":{}},{"cell_type":"code","source":"# number of subprocesses to use for data loading\nnum_workers = 2\n# how many samples per batch to load\nbatch_size = 32\n# percentage of training set to use as validation\nvalid_size = 0.2\ntest_size = 0.2","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.513878Z","iopub.execute_input":"2023-08-31T20:42:53.514965Z","iopub.status.idle":"2023-08-31T20:42:53.521292Z","shell.execute_reply.started":"2023-08-31T20:42:53.514922Z","shell.execute_reply":"2023-08-31T20:42:53.520159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choose the training and test datasets\nimg_dir = \"/kaggle/input/ocular-disease-recognition-odir5k/preprocessed_images\"\nlabels_dir = '/kaggle/working/labels_clean.csv'\n\nfull_data  = OcularDiseaseRecognition(csv_file=labels_dir, \n                                      root_dir=img_dir, \n                                      transform=transforms.Compose([Rescale(512),\n                                                                    ToTensor()])\n                                     )","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.522981Z","iopub.execute_input":"2023-08-31T20:42:53.524169Z","iopub.status.idle":"2023-08-31T20:42:53.542533Z","shell.execute_reply.started":"2023-08-31T20:42:53.524131Z","shell.execute_reply":"2023-08-31T20:42:53.541446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain training indices that will be used for validation\nnum_train = len(full_data)\nindices = list(range(num_train))\n\nnp.random.shuffle(indices)\n\nval_split = int(np.floor(valid_size * num_train))\ntest_split = int(np.floor(valid_size * num_train))\n\n\ntest_idx, valid_idx, train_idx = indices[:test_split], indices[test_split: test_split + val_split], indices[test_split + val_split:]","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.544063Z","iopub.execute_input":"2023-08-31T20:42:53.544791Z","iopub.status.idle":"2023-08-31T20:42:53.552653Z","shell.execute_reply.started":"2023-08-31T20:42:53.544674Z","shell.execute_reply":"2023-08-31T20:42:53.551519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.554168Z","iopub.execute_input":"2023-08-31T20:42:53.555271Z","iopub.status.idle":"2023-08-31T20:42:53.564641Z","shell.execute_reply.started":"2023-08-31T20:42:53.555235Z","shell.execute_reply":"2023-08-31T20:42:53.563501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\ntest_sampler  = SubsetRandomSampler(test_idx)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.566015Z","iopub.execute_input":"2023-08-31T20:42:53.567262Z","iopub.status.idle":"2023-08-31T20:42:53.573911Z","shell.execute_reply.started":"2023-08-31T20:42:53.567226Z","shell.execute_reply":"2023-08-31T20:42:53.572845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(full_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\n\nvalid_loader = torch.utils.data.DataLoader(full_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)\n\ntest_loader = torch.utils.data.DataLoader(full_data, batch_size=batch_size, \n    sampler=test_sampler, num_workers=num_workers)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.582161Z","iopub.execute_input":"2023-08-31T20:42:53.583059Z","iopub.status.idle":"2023-08-31T20:42:53.590252Z","shell.execute_reply.started":"2023-08-31T20:42:53.583024Z","shell.execute_reply":"2023-08-31T20:42:53.589178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_loader)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.59159Z","iopub.execute_input":"2023-08-31T20:42:53.592721Z","iopub.status.idle":"2023-08-31T20:42:53.602273Z","shell.execute_reply.started":"2023-08-31T20:42:53.592685Z","shell.execute_reply":"2023-08-31T20:42:53.601377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3 Visualize a Batch of Training Data","metadata":{}},{"cell_type":"code","source":"# helper function to un-normalize and display an image\ndef imshow(img):\n    img = img / 2 + 0.5  # unnormalize\n    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.603675Z","iopub.execute_input":"2023-08-31T20:42:53.604537Z","iopub.status.idle":"2023-08-31T20:42:53.611844Z","shell.execute_reply.started":"2023-08-31T20:42:53.604501Z","shell.execute_reply":"2023-08-31T20:42:53.611143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain one batch of training images\ndataiter = iter(train_loader)\nsample = next(dataiter)\n\nsample['image'].shape # (number of examples: 20, number of channels: 3, pixel sizes: 256x256)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:53.613332Z","iopub.execute_input":"2023-08-31T20:42:53.613911Z","iopub.status.idle":"2023-08-31T20:42:58.179979Z","shell.execute_reply.started":"2023-08-31T20:42:53.613874Z","shell.execute_reply":"2023-08-31T20:42:58.178658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\n# display 20 images\nfor idx in np.arange(batch_size):\n    ax = fig.add_subplot(2, 21, idx+1, xticks=[], yticks=[])\n    imshow(sample['image'][idx])","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:42:58.185174Z","iopub.execute_input":"2023-08-31T20:42:58.187441Z","iopub.status.idle":"2023-08-31T20:43:07.272714Z","shell.execute_reply.started":"2023-08-31T20:42:58.187398Z","shell.execute_reply":"2023-08-31T20:43:07.271673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Model Training","metadata":{}},{"cell_type":"code","source":"device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice = torch.device(device_name)\n\nprint(device_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:43:07.274232Z","iopub.execute_input":"2023-08-31T20:43:07.274807Z","iopub.status.idle":"2023-08-31T20:43:07.281009Z","shell.execute_reply.started":"2023-08-31T20:43:07.274772Z","shell.execute_reply":"2023-08-31T20:43:07.279916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state, is_best, filename='/kaggle/working/bt_resnet50_ckpt_v2.pth.tar'):\n    torch.save(state, filename)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:43:07.282814Z","iopub.execute_input":"2023-08-31T20:43:07.28343Z","iopub.status.idle":"2023-08-31T20:43:07.292081Z","shell.execute_reply.started":"2023-08-31T20:43:07.283397Z","shell.execute_reply":"2023-08-31T20:43:07.291067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1 Define Model Achitecture","metadata":{}},{"cell_type":"code","source":"# instantiate transfer learning model\nresnet_model = models.resnet50(pretrained=True)\n\n# set all parameters as trainable\nfor param in resnet_model.parameters():\n    param.requires_grad = True\n\n# get input of fc layer\nn_inputs = resnet_model.fc.in_features\n\n# redefine fc layer / top layer/ head for our classification problem\nresnet_model.fc = nn.Sequential(nn.Linear(n_inputs, 2048),\n                                nn.ReLU(),\n                                nn.Dropout(p=0.4),\n                                nn.Linear(2048, 2048),\n                                nn.ReLU(),\n                                nn.Dropout(p=0.4),\n                                nn.Linear(2048, 8),\n                                nn.LogSigmoid())\n\n# set all parameters of the model as trainable\nfor name, child in resnet_model.named_children():\n    for name2, params in child.named_parameters():\n        params.requires_grad = True\n\n\n# Disbribute the model to all GPU's\nresnet_model = nn.DataParallel(resnet_model)\n\n# set model to run on GPU or CPU absed on availibility\nresnet_model.to(device)\n\n# print the trasnfer learning NN model's architecture\nresnet_model","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:43:07.293541Z","iopub.execute_input":"2023-08-31T20:43:07.294152Z","iopub.status.idle":"2023-08-31T20:43:12.316705Z","shell.execute_reply.started":"2023-08-31T20:43:07.294118Z","shell.execute_reply":"2023-08-31T20:43:12.315658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Limit maximum memory usage to 1GB\n\n# torch.backends.cuda.reserved_memory = 4 * 1024 * 1024 * 1024\n# torch.backends.cuda.max_allocated_memory = 4 * 1024 * 1024 * 1024","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:43:12.31816Z","iopub.execute_input":"2023-08-31T20:43:12.318661Z","iopub.status.idle":"2023-08-31T20:43:12.325339Z","shell.execute_reply.started":"2023-08-31T20:43:12.318612Z","shell.execute_reply":"2023-08-31T20:43:12.32419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Define `Criterion` & `Optimizer`","metadata":{}},{"cell_type":"code","source":"# loss function\n# if GPU is available set loss function to use GPU\ncriterion = nn.CrossEntropyLoss().to(device)\n\n# optimizer\noptimizer = torch.optim.SGD(resnet_model.parameters(), momentum=0.9, lr=3e-4)\n\n\n# empty lists to store losses and accuracies\ntrain_losses = []\ntest_losses = []\ntrain_correct = []\ntest_correct = []","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:43:12.32657Z","iopub.execute_input":"2023-08-31T20:43:12.327189Z","iopub.status.idle":"2023-08-31T20:43:12.341031Z","shell.execute_reply.started":"2023-08-31T20:43:12.327154Z","shell.execute_reply":"2023-08-31T20:43:12.339998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Run the Traing Loop","metadata":{}},{"cell_type":"code","source":"# number of training iterations\nepochs = 32","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:43:12.342249Z","iopub.execute_input":"2023-08-31T20:43:12.342705Z","iopub.status.idle":"2023-08-31T20:43:12.357769Z","shell.execute_reply.started":"2023-08-31T20:43:12.342671Z","shell.execute_reply":"2023-08-31T20:43:12.356665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-08-31T20:43:12.3609Z","iopub.execute_input":"2023-08-31T20:43:12.361171Z","iopub.status.idle":"2023-08-31T20:43:12.368982Z","shell.execute_reply.started":"2023-08-31T20:43:12.361146Z","shell.execute_reply":"2023-08-31T20:43:12.368053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set training start time\nstart_time = time.time()\n\n# set best_prec loss value as 2 for checkpoint threshold\nbest_prec1 = 2\nis_best = False\n\n# empty batch variables\nb = None\ntrain_b = None\ntest_b = None\n\nscaler = GradScaler()\n\nfor i in range(epochs):\n    # empty training correct and test correct counter as 0 during every iteration\n    trn_corr = 0\n    tst_corr = 0\n    trn_loss = 0\n    tst_loss = 0\n    \n    # set epoch's starting time\n    e_start = time.time()\n    \n\n    # train in batches\n    for b, sample in enumerate(train_loader):\n        # set label as cuda if device is cuda\n        X, y = sample['image'].to(device, dtype=torch.float), sample['labels'].to(device, dtype=torch.float)\n        \n        # forward pass image sample\n        y_pred = resnet_model(X.view(-1, 3, 512, 512))\n\n        # calculate loss\n        loss = criterion(y_pred.float(), y.float())\n\n        trn_loss += loss.item()\n        # get argmax of predicted tensor, which is our label\n        predicted = torch.argmax(y_pred, dim=1).data\n        y = torch.argmax(y, dim=1).data\n\n        # if predicted label is correct as true label, calculate the sum for samples\n\n        batch_corr = (predicted == y).sum()\n        # increment train correct with correcly predicted labels per batch\n        trn_corr += batch_corr.item()\n        \n        # set optimizer gradients to zero\n        optimizer.zero_grad()\n        # Backpropagate with autocasting\n        # back propagate with loss\n        scaler.scale(loss).backward()\n        # perform optimizer step\n        scaler.step(optimizer)\n        scaler.update()\n      \n    # set epoch's end time\n    e_end = time.time()\n    \n    # print training metrics\n    print(f'Epoch {(i+1)} Batch {(b+1)}\\nAccuracy: {trn_corr*100/(b*batch_size):2.2f} %  Loss: {trn_loss/len(train_loader):2.4f}  Duration: {((e_end-e_start)/60):.2f} minutes') \n    \n    # some metrics storage for visualization\n    train_b = b\n    train_losses.append(trn_loss)\n    train_correct.append(trn_corr)\n\n    X, y = None, None\n\n    # validate using validation generator\n    # do not perform any gradient updates while validation\n    with torch.no_grad():\n        for b, sample in enumerate(valid_loader):\n            # set label as cuda if device is cuda\n            X, y = sample['image'].to(device, dtype=torch.float), sample['labels'].to(device, dtype=torch.float)\n\n            # forward pass image\n            y_val = resnet_model(X.view(-1, 3, 512, 512))\n\n            # get argmax of predicted tensor, which is our label\n            predicted = torch.argmax(y_val, dim=1).data\n            y = torch.argmax(y, dim=1).data\n\n            # increment test correct with correcly predicted labels per batch\n            tst_corr += (predicted == y).sum().item()\n\n            # get loss of validation set\n            loss = criterion(y_val.float(), y.long())\n            tst_loss += loss.item()\n            \n            \n    # print validation metrics\n    print(f'Validation Accuracy {tst_corr*100/(b*batch_size):2.2f}% Validation Loss: {tst_loss/len(valid_loader):2.4f}\\n')\n\n    # if current validation loss is less than previous iteration's validatin loss create and save a checkpoint\n    is_best = loss < best_prec1\n    best_prec1 = min(loss, best_prec1)\n    \n    if is_best:\n        save_checkpoint({\n                'epoch': i + 1,\n                'state_dict': resnet_model.state_dict(),\n                'best_prec1': best_prec1,\n            }, is_best)\n        \n        is_best = False\n\n    # some metrics storage for visualization\n    test_b  = b\n    test_losses.append(tst_loss)\n    test_correct.append(tst_corr)\n\n# set total training's end time\nend_time = time.time() - start_time    \n\n# print training summary\nprint(\"\\nTraining Duration {:.2f} minutes\".format(end_time/60))\nprint(\"GPU memory used : {} kb\".format(torch.cuda.memory_allocated()))\nprint(\"GPU memory cached : {} kb\".format(torch.cuda.memory_cached()))","metadata":{"execution":{"iopub.status.busy":"2023-08-31T21:53:06.589395Z","iopub.execute_input":"2023-08-31T21:53:06.589774Z","iopub.status.idle":"2023-09-01T01:33:10.823606Z","shell.execute_reply.started":"2023-08-31T21:53:06.589743Z","shell.execute_reply":"2023-09-01T01:33:10.821057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Validation accuracy: {test_correct[-1]*100/(test_b*batch_size):.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-09-01T01:36:13.97354Z","iopub.execute_input":"2023-09-01T01:36:13.973996Z","iopub.status.idle":"2023-09-01T01:36:13.980812Z","shell.execute_reply.started":"2023-09-01T01:36:13.973957Z","shell.execute_reply":"2023-09-01T01:36:13.979687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([t/test_b*batch_size for t in torch.tensor(test_correct).cpu()], label='Validation accuracy')\n\nplt.title('Accuracy Metrics')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-01T01:36:21.893158Z","iopub.execute_input":"2023-09-01T01:36:21.893565Z","iopub.status.idle":"2023-09-01T01:36:22.252519Z","shell.execute_reply.started":"2023-09-01T01:36:21.893531Z","shell.execute_reply":"2023-09-01T01:36:22.251515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}